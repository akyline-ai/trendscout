# backend/app/services/ai.py
import os
import requests
import base64
import numpy as np
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from anthropic import Anthropic

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ (—á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
_clip_model = None
_clip_processor = None
_claude_client = None

def get_claude_client():
    global _claude_client
    if not _claude_client:
        key = os.getenv("ANTHROPIC_API_KEY")
        if key:
            _claude_client = Anthropic(api_key=key)
    return _claude_client

def load_clip():
    global _clip_model, _clip_processor
    if _clip_model is None:
        print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ CLIP...")
        try:
            _clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            _clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            print("‚úÖ CLIP –∑–∞–≥—Ä—É–∂–µ–Ω.")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ CLIP: {e}")

def get_text_embedding(text: str) -> list:
    """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤ –≤–µ–∫—Ç–æ—Ä (—Å–ø–∏—Å–æ–∫ –∏–∑ 512 —á–∏—Å–µ–ª)"""
    load_clip()
    if not _clip_model or not text: return None
    try:
        inputs = _clip_processor(text=[text], return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = _clip_model.get_text_features(**inputs)
        return outputs.squeeze().numpy().tolist()
    except: return None

def get_image_embedding(image_url: str) -> list:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –≤ –≤–µ–∫—Ç–æ—Ä"""
    load_clip()
    if not _clip_model or not image_url: return None
    try:
        resp = requests.get(image_url, headers={"User-Agent": "Mozilla/5.0"}, stream=True, timeout=5)
        if resp.status_code != 200: return None
        image = Image.open(resp.raw)
        inputs = _clip_processor(images=image, return_tensors="pt")
        with torch.no_grad():
            outputs = _clip_model.get_image_features(**inputs)
        return outputs.squeeze().numpy().tolist()
    except: return None

def generate_trend_summary(description: str, views: int, cover_url: str = None) -> str:
    """–°–ø—Ä–∞—à–∏–≤–∞–µ—Ç —É Claude —Å—É—Ç—å —Ç—Ä–µ–Ω–¥–∞"""
    client = get_claude_client()
    if not client: return "AI Not Configured"

    prompt = f"–¢—Ä–µ–Ω–¥ TikTok. –û–ø–∏—Å–∞–Ω–∏–µ: {description}. –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {views}. –í —á–µ–º —Å—É—Ç—å —Ç—Ä–µ–Ω–¥–∞? –û—Ç–≤–µ—Ç—å –æ–¥–Ω–æ–π –∫–æ—Ä–æ—Ç–∫–æ–π —Ñ—Ä–∞–∑–æ–π."
    
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π, –µ—Å–ª–∏ –µ—Å—Ç—å
        messages = []
        if cover_url:
             # –õ–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –æ–ø—É—â–µ–Ω–∞ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ
             pass
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π)
        resp = client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=60,
            messages=[{"role": "user", "content": [{"type": "text", "text": prompt}]}]
        )
        return resp.content[0].text.strip()
    except Exception as e:
        print(f"‚ö†Ô∏è AI Summary Error: {e}")
        return "Pending"